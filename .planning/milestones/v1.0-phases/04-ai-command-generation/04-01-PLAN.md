---
phase: 04-ai-command-generation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src-tauri/Cargo.toml
  - src-tauri/src/commands/ai.rs
  - src-tauri/src/commands/mod.rs
  - src-tauri/src/lib.rs
autonomous: true
requirements: [AICG-01, AICG-02]

must_haves:
  truths:
    - "Rust stream_ai_response command compiles and accepts query, model, context, history, and on_token Channel parameters"
    - "SSE tokens from xAI /v1/chat/completions are parsed and forwarded through the Tauri IPC Channel"
    - "API key is read from macOS Keychain inside Rust, never passed from frontend"
    - "Two distinct system prompts exist: terminal mode (command-only) and assistant mode (conversational)"
    - "Session history messages are included in the API request body up to 7 turns"
    - "10-second hard timeout prevents hung connections"
  artifacts:
    - path: "src-tauri/src/commands/ai.rs"
      provides: "stream_ai_response Tauri command with SSE parsing"
      exports: ["stream_ai_response"]
    - path: "src-tauri/Cargo.toml"
      provides: "eventsource-stream, futures-util, tokio deps; stream feature on tauri-plugin-http"
      contains: "eventsource-stream"
  key_links:
    - from: "src-tauri/src/commands/ai.rs"
      to: "src-tauri/src/commands/keychain.rs"
      via: "keyring::Entry::new() for API key retrieval"
      pattern: "keyring::Entry"
    - from: "src-tauri/src/commands/ai.rs"
      to: "https://api.x.ai/v1/chat/completions"
      via: "reqwest POST with stream:true"
      pattern: "chat/completions"
    - from: "src-tauri/src/commands/ai.rs"
      to: "tauri::ipc::Channel<String>"
      via: "on_token.send(token) per SSE chunk"
      pattern: "on_token.send"
---

<objective>
Create the Rust backend command `stream_ai_response` that receives a user query, builds context-aware messages with two-mode system prompts (terminal vs assistant), calls xAI /v1/chat/completions with streaming enabled, parses SSE chunks via eventsource-stream, and forwards each token to the frontend through a Tauri IPC Channel.

Purpose: This is the core AI integration -- connecting the overlay to xAI's API while keeping the API key securely in Rust/Keychain.

Output: `src-tauri/src/commands/ai.rs` with `stream_ai_response` Tauri command, updated Cargo.toml with streaming dependencies, command registered in lib.rs.
</objective>

<execution_context>
@/Users/lakshmanturlapati/.claude/get-shit-done/workflows/execute-plan.md
@/Users/lakshmanturlapati/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-ai-command-generation/04-CONTEXT.md
@.planning/phases/04-ai-command-generation/04-RESEARCH.md
@.planning/phases/02-settings-configuration/02-01-SUMMARY.md
@.planning/phases/03-terminal-context-reading/03-04-SUMMARY.md
@src-tauri/src/commands/xai.rs
@src-tauri/src/commands/keychain.rs
@src-tauri/src/commands/terminal.rs
@src-tauri/src/terminal/mod.rs
@src-tauri/Cargo.toml
@src-tauri/src/lib.rs
@src-tauri/src/commands/mod.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add streaming dependencies and create ai.rs with stream_ai_response command</name>
  <files>
    src-tauri/Cargo.toml
    src-tauri/src/commands/ai.rs
    src-tauri/src/commands/mod.rs
    src-tauri/src/lib.rs
  </files>
  <action>
**Cargo.toml changes:**
- Change `tauri-plugin-http = "2"` to `tauri-plugin-http = { version = "2", features = ["stream"] }` to enable `bytes_stream()` on reqwest Response.
- Add `eventsource-stream = "0.2"` for SSE line parsing.
- Add `futures-util = "0.3"` for `StreamExt::next()`.
- Add `tokio = { version = "1", features = ["time"] }` for `tokio::time::timeout`.

**Create src-tauri/src/commands/ai.rs:**

The command signature:
```rust
#[tauri::command]
pub async fn stream_ai_response(
    query: String,
    model: String,
    context_json: String,  // JSON-serialized AppContext from frontend
    history: Vec<ChatMessage>,  // Previous turns [{role, content}]
    on_token: tauri::ipc::Channel<String>,
) -> Result<(), String>
```

Where `ChatMessage` is:
```rust
#[derive(serde::Deserialize)]
pub struct ChatMessage {
    pub role: String,
    pub content: String,
}
```

Implementation steps inside the command:

1. **Read API key from Keychain** using the same pattern as keychain.rs:
   ```rust
   let entry = keyring::Entry::new("com.lakshmanturlapati.cmd-k", "api-key")
       .map_err(|e| format!("Keyring error: {}", e))?;
   let api_key = entry.get_password()
       .map_err(|_| "No API key configured. Open Settings to add one.".to_string())?;
   ```

2. **Parse the context** from `context_json` string into an AppContext-like struct (or use serde_json::Value) to build the user message.

3. **Determine mode**: If the parsed context has `terminal.shell_type` set (non-null), use terminal mode. Otherwise, use assistant mode.

4. **Build the system prompt**:
   - Terminal mode system prompt (stored as a const string in ai.rs):
     ```
     You are a terminal command generator for macOS. Given the user's task description and terminal context, output ONLY the exact command(s) to run. No explanations, no markdown, no code fences. Just the raw command(s). If multiple commands are needed, separate them with && or use pipes. Prefer common POSIX tools (grep, find, sed, awk) over modern alternatives (rg, fd, jq). The user is on macOS with {shell_type} shell.
     ```
   - Assistant mode system prompt:
     ```
     You are a concise assistant accessed via a macOS overlay. Answer in 2-3 sentences maximum. Be direct and helpful. No markdown formatting, no code fences unless the user explicitly asks for code.
     ```

5. **Build the user message** using `build_user_message()` helper function:
   - If terminal mode, include: App name, Shell type, CWD, Running process, Terminal output (last 25 lines), Console last line (if browser with DevTools), then the task.
   - If assistant mode, include: App name (if available), Console last line (if browser), then the question.
   - Format per CONTEXT.md decisions (see RESEARCH.md `build_user_message` example).

6. **Build messages array**: system prompt + history (capped at last 7 pairs = 14 messages) + current user message.

7. **Build the HTTP request body**:
   ```rust
   let body = serde_json::json!({
       "model": model,
       "messages": messages,
       "stream": true,
       "temperature": 0.1
   }).to_string();
   ```

8. **Make the HTTP request** using the established pattern from xai.rs:
   ```rust
   let client = tauri_plugin_http::reqwest::Client::new();
   let response = client
       .post("https://api.x.ai/v1/chat/completions")
       .header("Authorization", format!("Bearer {}", api_key))
       .header("Content-Type", "application/json")
       .body(body)
       .send()
       .await
       .map_err(|e| format!("Network error: {}", e))?;
   ```

9. **Check HTTP status** before streaming:
   - 200: proceed to SSE parsing
   - 401: return `Err("Authentication failed. Check your API key in Settings.".to_string())`
   - 429: return `Err("Rate limit exceeded. Please wait a moment and try again.".to_string())`
   - Other: return `Err(format!("API error ({}). Try again.", status))`

10. **Parse SSE stream with 10-second timeout**:
    ```rust
    use eventsource_stream::Eventsource;
    use futures_util::StreamExt;

    let mut stream = response.bytes_stream().eventsource();
    let timeout_duration = tokio::time::Duration::from_secs(10);

    let result = tokio::time::timeout(timeout_duration, async {
        while let Some(event) = stream.next().await {
            match event {
                Ok(event) => {
                    let data = event.data;
                    if data == "[DONE]" { break; }
                    if let Ok(chunk) = serde_json::from_str::<serde_json::Value>(&data) {
                        if let Some(token) = chunk["choices"][0]["delta"]["content"].as_str() {
                            if !token.is_empty() {
                                on_token.send(token.to_string())
                                    .map_err(|e| format!("Channel error: {}", e))?;
                            }
                        }
                    }
                }
                Err(e) => {
                    return Err(format!("Stream error: {}", e));
                }
            }
        }
        Ok::<(), String>(())
    }).await;

    match result {
        Ok(Ok(())) => Ok(()),
        Ok(Err(e)) => Err(e),
        Err(_) => Err("Request timed out. Try again.".to_string()),
    }
    ```

**Register the command:**
- In `src-tauri/src/commands/mod.rs`: add `pub mod ai;`
- In `src-tauri/src/lib.rs`: add `commands::ai::stream_ai_response` to the imports and to `generate_handler![]`

**Important notes:**
- Use the same keyring service name as keychain.rs: `"com.lakshmanturlapati.cmd-k"` with username `"api-key"`.
- Do NOT use `.json()` on the request builder (not available in tauri-plugin-http re-export per established pattern).
- The `on_token` parameter name in Rust maps to `onToken` in TypeScript (Tauri's automatic camelCase conversion).
- Use `eprintln!` for debug logging consistent with rest of codebase.
  </action>
  <verify>
Run `cd src-tauri && cargo check` -- must compile with zero errors. Warnings about unused imports or dead code in ai.rs are acceptable since the frontend caller does not exist yet (Plan 02 wires it).
  </verify>
  <done>
- ai.rs exists with stream_ai_response command that reads API key from Keychain, builds two-mode system prompts, constructs context-aware messages with history, POSTs to xAI with stream:true, parses SSE via eventsource-stream, and sends tokens via Channel
- Cargo.toml has eventsource-stream, futures-util, tokio[time], and tauri-plugin-http with stream feature
- Command registered in mod.rs and lib.rs generate_handler
- cargo check passes
  </done>
</task>

</tasks>

<verification>
1. `cd src-tauri && cargo check` -- zero compilation errors
2. `cd src-tauri && cargo clippy -- -W clippy::all` -- no errors in new ai.rs code (pre-existing warnings acceptable)
3. Verify `stream_ai_response` appears in `generate_handler![]` in lib.rs
4. Verify keyring service name matches keychain.rs exactly
</verification>

<success_criteria>
- Rust backend compiles cleanly with all streaming dependencies
- stream_ai_response command has correct signature with Channel<String> parameter
- Two system prompt modes (terminal/assistant) exist based on shell_type presence
- SSE parsing handles [DONE] sentinel before JSON parse
- 10-second timeout wraps the streaming loop
- API key read from Keychain, never accepted as a parameter from frontend
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-command-generation/04-01-SUMMARY.md`
</output>
